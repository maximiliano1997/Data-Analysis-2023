{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bdaec30",
   "metadata": {},
   "source": [
    "### Partes 1 y 2: \n",
    "\n",
    "\n",
    "##### (Part1) Getting Started with Data Analysis - Installation and Loading Data\n",
    "##### (Part2) - DataFrame and Series Basics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pdimport pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/survey_results_public.csv')\n",
    "schema_df = pd.read_csv('../data/survey_results_schema.csv')\n",
    "\n",
    "df\n",
    "\n",
    "schema_df\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.info()\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.min_rows', 100)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "\n",
    "df\n",
    "\n",
    "df.tail(10)\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "df.iloc[1:,2]\n",
    "\n",
    "df.loc[10, ['CompTotal','CompFreq','Currency','Employment']]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1f7e9cc",
   "metadata": {},
   "source": [
    "# Partes 3 y 4: \n",
    "\n",
    "##### (3) Indexes: How to Set, Reset, and Use Indexes\n",
    "##### (4)Filtering: Using Conditionals to Filter Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (3) Indexes: How to Set, Reset, and Use Indexes\n",
    "\n",
    "people = {\n",
    "\"first\": [\"Corey\",\"Jane\",\"Jhon\"],\n",
    "\"last\": [\"Schafer\",\"Doe\",\"Doe\"],\n",
    "\"email\": [\"CoreyMSchager@gmail.com\",\"loco@gamil.com\",\"lineal@gmail.com\"],\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(people)\n",
    "\n",
    "df\n",
    "\n",
    "df['email']\n",
    "\n",
    "df.set_index('email', inplace=True)\n",
    "\n",
    "df\n",
    "\n",
    "df.index\n",
    "\n",
    "df.loc['lineal@gmail.com', 'last']\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Working with stackoverflow data \n",
    "\n",
    "dfs = pd.read_csv('../data/survey_results_public.csv', index_col='ResponseId')\n",
    "schema_dfs = pd.read_csv('../data/survey_results_schema.csv', index_col='qname')\n",
    "\n",
    "pd.set_option('display.max_columns',79)\n",
    "pd.set_option('display.max_rows',73)\n",
    "\n",
    "dfs.head()\n",
    "\n",
    "dfs.set_index('ResponseId', inplace=True)\n",
    "\n",
    "schema_dfs\n",
    "\n",
    "schema_dfs.loc['Employment', 'question']\n",
    "\n",
    "schema_dfs.sort_index()\n",
    "\n",
    "# ------------------------------END--------------------------------\n",
    "\n",
    "## (4)Filtering: Using Conditionals to Filter Rows and Columns\n",
    "\n",
    "df\n",
    "\n",
    "filt = (df['last'] == 'Doe') & (df['first'] == 'Jane')\n",
    "\n",
    "df[filt]\n",
    "\n",
    "df.loc[-filt, 'email']\n",
    "\n",
    "# Working with Stackoverflow Data \n",
    "\n",
    "dfs.head()\n",
    "\n",
    "high_salary = (dfs['CompTotal'] > 200000)\n",
    "\n",
    "dfs.loc[high_salary, ['CompTotal', 'CompFreq', 'Currency', 'Country', 'LanguageHaveWorkedWith']]\n",
    "\n",
    "countries = ['Uruguay','Argentina','Paraguay']\n",
    "search = (dfs['Country'].isin(countries))\n",
    "\n",
    "dfs.loc[search, ['Country' ,'CompTotal', 'Currency', 'CompFreq', 'LanguageHaveWorkedWith']]\n",
    "\n",
    "pd.set_option('display.max_columns', 70)\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.min_rows', 600)\n",
    "\n",
    "containFilter = dfs['LanguageHaveWorkedWith'].str.contains('Python', na=False)\n",
    "\n",
    "containFilter\n",
    "\n",
    "dfs.loc[containFilter, 'LanguageHaveWorkedWith']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f1beca",
   "metadata": {},
   "source": [
    "# Partes 5 y 6: \n",
    "\n",
    "##### (5): Updating Rows and Columns - Modifying Data Within DataFrames\n",
    "##### (Part 6) Add or Remove Rows and Columns From DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c96620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (5): Updating Rows and Columns - Modifying Data Within DataFrames\n",
    "\n",
    "#### In this Python Programming, we will be learning how to modify the data within our DataFrames. We will use some of the filtering techniques we learned in the last video to update values conditionally, and we will also be learning how to use the apply, map, and applymap method. Let's get started...\n",
    "\n",
    "people = {\n",
    "\"first\": [\"Corey\",\"Jane\",\"Jhon\"],\n",
    "\"last\": [\"Schafer\",\"Doe\",\"Doe\"],\n",
    "\"email\": [\"CoreyMSchager@gmail.com\",\"loco@gamil.com\",\"lineal@gmail.com\"],\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(people)\n",
    "\n",
    "df\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.columns = ['first_name', 'last_name', 'email']\n",
    "\n",
    "df\n",
    "\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "df\n",
    "\n",
    "df.columns = df.columns.str.replace(' ', \"_\")\n",
    "df\n",
    "\n",
    "df.rename(columns={'first_name': 'first', 'last_name': 'last'}, inplace=True)\n",
    "\n",
    "df.loc[2] = ['Imanol','Aguer','imanolaguer1@gmail.com']\n",
    "df\n",
    "\n",
    "df.loc[2, ['last','email']] = ['Doe','lineal@gmail.com']\n",
    "df\n",
    "\n",
    "# This is a Common error when try to change data\n",
    "filt = (df['email'] == 'lineal@gmail.com')\n",
    "df[filt]['last'] = 'Smith'\n",
    "\n",
    "# Asi se soluciona... con La manera correcta de modificar data en el DataFrame\n",
    "filt = (df['email'] == 'lineal@gmail.com')\n",
    "df.loc[filt, 'last'] = ['Pnachito']\n",
    "\n",
    "df['email'] = df['email'].str.lower()\n",
    "df\n",
    "\n",
    "#### Recuerda estos metodos\n",
    "##### 1- apply 2. map 3. applymap 4.replace\n",
    "\n",
    "## apply method \n",
    "\n",
    "df['email'].apply(len)\n",
    "\n",
    "def update_email(email):\n",
    "    return email.upper()\n",
    "\n",
    "df['email'] = df['email'].apply(update_email)\n",
    "# df\n",
    "\n",
    "df['email'] = df['email'].apply(lambda x: x.lower()) # Esta es una funcion Lambda\n",
    "df\n",
    "\n",
    "df.apply(len)\n",
    "\n",
    "df.apply(pd.Series.min)\n",
    "\n",
    "### applymap method\n",
    "\n",
    "df.applymap(len) # <---- te dara Len() de cada elemento individual del DataFrame\n",
    "\n",
    "df.applymap(str.upper)\n",
    "\n",
    "### map method \n",
    "\n",
    "df['first'].map({'Corey': 'Lionel','Jane': 'Cristiano'}) #atencion: no pone los cambios pernanentes\n",
    "\n",
    "df['first'] = df['first'].replace({'Corey': 'Lionel', 'Jane':'Cristiano'}) #Atencion: aqui si los cambios quedan\n",
    "df\n",
    "\n",
    "# Working with Stackoverflow Data \n",
    "\n",
    "dfs = pd.read_csv('../data/survey_results_public.csv')\n",
    "schema_dfs = pd.read_csv('../data/survey_results_schema.csv')\n",
    "\n",
    "dfs.head(20)\n",
    "\n",
    "dfs['Currency']\n",
    "\n",
    "dfs.rename(columns={'Currency': 'LocalCurrency'}, inplace=True)\n",
    "\n",
    "dfs['LocalCurrency']\n",
    "\n",
    "dfs['SurveyEase']\n",
    "\n",
    "dfs['SurveyEase'] = dfs['SurveyEase'].map({'Easy': 'Facil', 'Difficult': 'Dificil', 'Neither easy nor difficult':'ni pedos'})\n",
    "# A veces es conveniente usar metodo .replace en vez de .map\n",
    "\n",
    "dfs.head()\n",
    "\n",
    "# (Part 6) Add or Remove Rows and Columns From DataFrames \n",
    "\n",
    "\n",
    "### This is how we add columns to our DataFrame\n",
    "\n",
    "df['first'] + \" \" + df['last']\n",
    "\n",
    "df[\"Full_Name\"] = df['first'] + \" \" + df['last'] # <--- Created a new column\n",
    "\n",
    "df\n",
    "\n",
    "### This is how we remove columns to our DataFrame \n",
    "\n",
    "df.drop(columns=['first', 'last'], inplace=True) # <--- Borra las columnas first y last\n",
    "df\n",
    "# Hey, Imanol other thins!\n",
    "# You can simply delete columns by using \"del\" function.\n",
    "# For example: del df['full_name']\n",
    "\n",
    "df['Full_Name'].str.split(\" \", expand=True)\n",
    "\n",
    "df[['first','last']] = df['Full_Name'].str.split(\" \", expand=True)\n",
    "\n",
    "df\n",
    "\n",
    "# df.drop(columns=['Full_Name'])\n",
    "\n",
    "# df.append({'first': 'Tony'}) # Append was deprecated in new version of pandas, now are used concat\n",
    "df = pd.concat([df, pd.DataFrame([{'first': 'Tony'}])]) # <-- Add single row / ignore index\n",
    "\n",
    "df\n",
    "\n",
    "## Second DataFrame... people2\n",
    "\n",
    "people2 = {\n",
    "    \"first\": [\"George\", \"Martin\"],\n",
    "    \"last\": [\"Hotz\", \"Shkreli\"],\n",
    "    \"email\": [\"Hotz@gmail.com\", \"Shkreli@gamil.com\"],\n",
    "}\n",
    "df2 = pd.DataFrame(people2)\n",
    "df2\n",
    "\n",
    "df = pd.concat([df,df2], ignore_index=True, sort=False) # <--- This replaces the method append of min 11:30\n",
    "\n",
    "df\n",
    "\n",
    "df.drop(index=(3), inplace=True) # <--- remover un row individual\n",
    "df\n",
    "\n",
    "df.drop(index=[4,5], inplace=True) # <--- remover un grupo de rows\n",
    "df\n",
    "\n",
    "df\n",
    "\n",
    "df.drop(index=df[df['last'] == 'Doe'].index, inplace=True) # <--- Remover rows con un condicional\n",
    "# Tambien recuerda que puedes almacenar el condicional en una variable y solo pasar la variable al drop index=[filt]\n",
    "filt = df['last'] == 'Doe' # <-- Usando\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "776dfa63",
   "metadata": {},
   "source": [
    "# Partes 7 y 8: \n",
    "\n",
    "##### (Part 7): Sorting Data\n",
    "##### (Part 8) - Grouping and Aggregating - Analyzing and Exploring Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Part 7): Sorting Data\n",
    "\n",
    "people = {\n",
    "    \"first\": [\"Corey\", \"Jane\", \"Jhon\",\"Adam\"],\n",
    "    \"last\": [\"Schafer\", \"Doe\", \"Doe\", \"Doe\"],\n",
    "    \"email\": [\"CoreyMSchager@gmail.com\", \"loco@gamil.com\", \"lineal@gmail.com\", \"A@email.com\"],\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(people)\n",
    "df\n",
    "\n",
    "df.sort_values(by='last', ascending=False)\n",
    "\n",
    "df.sort_values(by=['last','first'], ascending=False)\n",
    "\n",
    "df.sort_values(by=['last','first'], ascending=[False, True], inplace=True)\n",
    "df\n",
    "\n",
    "\n",
    "df.sort_index()\n",
    "\n",
    "df['last'].sort_values()\n",
    "\n",
    "## Working With StakOverflow Data\n",
    "\n",
    "dfs = pd.read_csv('../data/survey_results_public.csv')\n",
    "schema_dfs = pd.read_csv('../data/survey_results_schema.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', 79)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.min_rows', 50)\n",
    "dfs.head()\n",
    "\n",
    "dfs.sort_values(by=['Country', 'Currency', 'CompTotal'], ascending=[True, False, False], inplace=True)\n",
    "\n",
    "dfs[['Country', 'Currency', 'CompTotal']].head(150)\n",
    "\n",
    "\n",
    "dfs['CompTotal'].nlargest(10)\n",
    "\n",
    "dfs.nlargest(10, 'CompTotal')\n",
    "\n",
    "dfs.nsmallest(10, 'CompTotal')\n",
    "\n",
    "## (Part 8) - Grouping and Aggregating - Analyzing and Exploring Your Data\n",
    "\n",
    "dfs.head()\n",
    "\n",
    "dfs['CompTotal'].head(15)\n",
    "\n",
    "dfs['CompTotal'].median()\n",
    "\n",
    "dfs.describe()\n",
    "\n",
    "dfs['CompTotal'].count()\n",
    "\n",
    "dfs['SOAccount']\n",
    "\n",
    "dfs['SOAccount'].value_counts()\n",
    "\n",
    "dfs['Sexuality']\n",
    "\n",
    "schema_dfs.loc[47]\n",
    "\n",
    "dfs['Sexuality'].value_counts(normalize=True)\n",
    "\n",
    "dfs['Country'].value_counts()\n",
    "\n",
    "country_grp = dfs.groupby(dfs['Country'])\n",
    "\n",
    "country_grp.get_group('Argentina')\n",
    "\n",
    "filt = dfs['Country'] == 'Argentina'\n",
    "dfs.loc[filt]['Sexuality'].value_counts()\n",
    "\n",
    "country_grp['Sexuality'].value_counts().loc['Argentina']\n",
    "\n",
    "country_grp['CompTotal'].median().loc['Argentina']\n",
    "\n",
    "country_grp['CompTotal'].agg(['median','mean']).loc['Canada']\n",
    "\n",
    "country_grp['LanguageHaveWorkedWith'].str.contains('Python').sum()\n",
    "\n",
    "country_grp['LanguageHaveWorkedWith'].apply(\n",
    "   lambda x: x.str.contains('Python').sum())\n",
    "\n",
    "country_respondents = dfs['Country'].value_counts()\n",
    "country_respondents\n",
    "\n",
    "country_uses_python = country_grp['LanguageHaveWorkedWith'].apply(lambda x: x.str.contains('Python').sum())\n",
    "country_uses_python\n",
    "\n",
    "python_df = pd.concat([country_respondents, country_uses_python], axis='columns', sort=False)\n",
    "python_df\n",
    "\n",
    "python_df.rename(columns={'count':'cantidadRespondieron', 'LanguageHaveWorkedWith':'LosQueSabenPyhon'}, inplace=True)\n",
    "\n",
    "python_df\n",
    "\n",
    "python_df['PerKnowPython'] = (python_df['LosQueSabenPyhon'] / python_df['cantidadRespondieron']) * 100\n",
    "\n",
    "python_df\n",
    "\n",
    "python_df.sort_values(by='PerKnowPython', ascending=False, inplace=True)\n",
    "\n",
    "python_df.head(50)\n",
    "\n",
    "python_df.loc['Argentina']\n",
    "\n",
    "# --------------- END --------------- \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5021be88",
   "metadata": {},
   "source": [
    "## Partes 9 y 10\n",
    "\n",
    "#### (Part 9) - Cleaning Data - Casting Datatypes and Handling Missing Values\n",
    "#### (Part 10) - Working with Dates and Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "people = {\n",
    "    'first': ['Corey', 'Jane', 'John', 'Chris', np.nan, None, 'NA'],\n",
    "    'last': ['Schafer', 'Doe', 'Doe', 'Schafer', np.nan, np.nan, 'Missing'],\n",
    "    'email': ['CoreyMSchafer@gmail.com', 'JaneDoe@email.com', 'JohnDoe@email.com', None, np.nan, 'Anonymous@email.com', 'NA'],\n",
    "    'age': ['33', '55', '63', '36', None, None, 'Missing']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(people)\n",
    "\n",
    "df.replace('NA', np.nan, inplace=True)\n",
    "\n",
    "df.replace('Missing', np.nan, inplace=True)\n",
    "df\n",
    "\n",
    "df.dropna()\n",
    "\n",
    "df.dropna(axis='index', how='all', subset=['last', 'email'])\n",
    "\n",
    "df.isna()\n",
    "\n",
    "df.fillna(0)\n",
    "\n",
    "df['age'].mean()\n",
    "\n",
    "df['age'] = df['age'].astype(float)\n",
    "df.dtypes\n",
    "\n",
    "df['age'].mean()\n",
    "\n",
    "# Working with StackOverflow Data\n",
    "\n",
    "def d_parser(x): return datetime.strptime(x, '%Y-%m-%d %I-%p')\n",
    "\n",
    "\n",
    "dfs = pd.read_csv('../data/survey_results_public.csv', index_col='ResponseId')\n",
    "dfs_schema = pd.read_csv(\n",
    "    '../data/survey_results_schema.csv', index_col='qname')\n",
    "\n",
    "pd.set_option('display.max_columns', 85)\n",
    "pd.set_option('display.max_rows', 85)\n",
    "\n",
    "dfs.head()\n",
    "\n",
    "dfs['YearsCode'].head(10)\n",
    "\n",
    "dfs['YearsCode'].unique()\n",
    "\n",
    "dfs['YearsCode'].replace('Less than 1 year', 0, inplace=True)\n",
    "\n",
    "dfs['YearsCode'].replace('More than 50 years', 51, inplace=True)\n",
    "\n",
    "dfs['YearsCode'] = dfs['YearsCode'].astype(float)\n",
    "\n",
    "dfs['YearsCode'].mean()\n",
    "\n",
    "dfs['YearsCode'].median()\n",
    "\n",
    "# (Part 10) - Working with Dates and Time Series Data\n",
    "\n",
    "def d_parser(x): return datetime.strptime(x, '%Y-%m-%d %I-%p')\n",
    "\n",
    "\n",
    "dft = pd.read_csv('../data/ETH_1h.csv',\n",
    "                  parse_dates=['Date'], date_parser=d_parser)\n",
    "\n",
    "dft.head()\n",
    "\n",
    "# dft.loc[0, 'Date'].day_name()\n",
    "# dft['Date'] = pd.to_datetime(dft['Date'], format='%Y-%m-%d %I-%p')\n",
    "# dft['Date']\n",
    "\n",
    "dft.loc[0, 'Date'].day_name()\n",
    "\n",
    "dft['Date'].dt.day_name()\n",
    "\n",
    "dft['DayOfWeek'] = dft['Date'].dt.day_name()\n",
    "dft\n",
    "\n",
    "dft['Date'].min()\n",
    "\n",
    "dft['Date'].max()\n",
    "\n",
    "dft['Date'].max() - dft['Date'].min()\n",
    "\n",
    "filt = (dft['Date'] >= pd.to_datetime('2019-01-01')\n",
    "        ) & (dft['Date'] < pd.to_datetime('2020-01-01'))\n",
    "dft.loc[filt]\n",
    "\n",
    "dft.set_index('Date', inplace=True)\n",
    "dft\n",
    "\n",
    "dft.loc['2019']\n",
    "\n",
    "dft['2020-01':'2020-02']\n",
    "\n",
    "dft['2020-01':'2020-02']['Close'].mean()\n",
    "\n",
    "dft['2020-01-01':'2020-01-01']['High'].max()\n",
    "highs = dft['High'].resample('D').max()\n",
    "highs['2020-01-01']\n",
    "\n",
    "# %matplotlib inline\n",
    "# highs.plot()\n",
    "\n",
    "dft.resample('W').mean()\n",
    "dft\n",
    "\n",
    "dft.resample('W').agg(\n",
    "    {'Close': 'mean', 'High': 'max', 'Low': 'min', 'Volume': 'sum'})\n",
    "\n",
    "##  ########### END ########### ##\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7af5d648",
   "metadata": {},
   "source": [
    "## Parte 11\n",
    "\n",
    "#### (Part 11) - Reading and Writing Data to Different Sources - Excel, JSON, SQL, Etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/survey_results_public.csv', index_col='ResponseId')\n",
    "schema_df = pd.read_csv('../data/survey_results_schema.csv', index_col='qname')\n",
    "\n",
    "pd.set_option('display.max_columns', 85)\n",
    "pd.set_option('display.max_rows', 85)\n",
    "\n",
    "df.head()\n",
    "\n",
    "filt = (df['Country'] == 'India')\n",
    "\n",
    "india_df = df.loc[filt]\n",
    "india_df.head()\n",
    "\n",
    "india_df.to_csv('modified.csv')\n",
    "india_df.to_csv('modified.tsv', sep='>')\n",
    "\n",
    "india_df.to_excel('excel_writer.xlsx')\n",
    "\n",
    "!pip install openpyxl\n",
    "india_df.to_excel('modified.xlsx')\n",
    "test = pd.read_excel('modified.xlsx', index_col='ResponseId')\n",
    "test.head()\n",
    "\n",
    "india_df.to_json('modified.json', orient='records', lines=True)\n",
    "test = pd.read_json('modified.json', orient='records', lines=True)\n",
    "test.head()\n",
    "\n",
    "\n",
    "!pip install mysql-connector-python\n",
    "!pip install SQLAlchemy\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:root@localhost/usingdata')\n",
    "connection = engine.connect()\n",
    "tabla = 'sample'\n",
    "india_df.to_sql(name=tabla, con=connection, if_exists='append', index=False)\n",
    "sql_df = pd.read_sql(tabla, con=connection)\n",
    "sql_df.head()\n",
    "sql_df = pd.read_sql_query('SELECT * FROM sample', con=connection)\n",
    "sql_df.head()\n",
    "\n",
    "posts_df = pd.read_json(\n",
    "    'https://raw.githubusercontent.com/CoreyMSchafer/code_snippets/master/Python/Flask_Blog/snippets/posts.json')\n",
    "posts_df.head()\n",
    "\n",
    "########################### END ########################\n",
    "# In this Python Programming video, we will be learning how to load and save data using multiple different sources. We will learn how to read/write data to CSV, JSON, Excel, SQL, and more. This covers the vast majority of formats you'll see in the data science field and will be extremely useful to know. Let's get started...\n",
    "\n",
    "# Video Timestamps:\n",
    "# Read CSV - 0: 56\n",
    "# Write CSV - 3: 20\n",
    "# Write TSV - 4: 40\n",
    "# Read TSV - 6: 00\n",
    "# Write Excel - 6: 15\n",
    "# Read Excel - 10: 42 (Start at 6: 15 to see installed packages)\n",
    "# Write JSON - 12: 18\n",
    "# Read JSON - 15: 41\n",
    "# Write SQL - 16: 59\n",
    "# Read SQL - 24: 57 (Start at 16: 59 to see installed packages)\n",
    "\n",
    "# The code for this video can be found at:\n",
    "# bit.ly/Pandas-11\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
